#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Reddit ç†±é–€æ–‡ç« çˆ¬èŸ² - Python ç‰ˆæœ¬
ä½¿ç”¨ Selenium æ¨¡æ“¬ç€è¦½å™¨ä¾†æŠ“å– Reddit JSON API è³‡æ–™
"""

import json
import time
import random
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from fake_useragent import UserAgent

class RedditUrl:
    """Reddit URL é…ç½®é¡"""
    def __init__(self, url: str, filename: str, description: str):
        self.url = url
        self.filename = filename
        self.description = description

# Reddit å­ç‰ˆå¡Šé…ç½®
REDDIT_URLS = [
    RedditUrl(
        url='https://www.reddit.com/r/all/hot.json?limit=50',
        filename='data/reddit-all-hot.json',
        description='Reddit r/all ç†±é–€æ–‡ç« '
    ),
    RedditUrl(
        url='https://www.reddit.com/r/Taiwanese/hot.json?limit=50',
        filename='data/reddit-taiwanese-hot.json',
        description='Reddit r/Taiwanese ç†±é–€æ–‡ç« '
    ),
    RedditUrl(
        url='https://www.reddit.com/r/China_irl/hot.json?limit=50',
        filename='data/reddit-china-irl-hot.json',
        description='Reddit r/China_irl ç†±é–€æ–‡ç« '
    )
]

def setup_driver():
    """è¨­å®š Chrome WebDriver"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-setuid-sandbox')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-features=VizDisplayCompositor')
    
    # è¨­å®šéš¨æ©Ÿ User-Agent
    ua = UserAgent()
    user_agent = ua.random
    options.add_argument(f'--user-agent={user_agent}')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    # ç§»é™¤ webdriver ç—•è·¡
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

def fetch_reddit_data_with_selenium(url: str) -> Optional[Dict]:
    """ä½¿ç”¨ Selenium ç²å– Reddit JSON è³‡æ–™"""
    driver = setup_driver()
    
    try:
        print(f"ğŸ”— æ­£åœ¨å­˜å–: {url}")
        
        # éš¨æ©Ÿå»¶é²é¿å…è¢«åµæ¸¬
        delay = random.uniform(2, 5)
        print(f"â³ éš¨æ©Ÿå»¶é² {delay:.2f} ç§’...")
        time.sleep(delay)
        
        # å‰å¾€ Reddit JSON API
        driver.get(url)
        
        # ç­‰å¾…é é¢è¼‰å…¥
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            print("âš ï¸ é é¢è¼‰å…¥è¶…æ™‚")
            return None
        
        # é¡å¤–ç­‰å¾…ç¢ºä¿å…§å®¹å®Œå…¨è¼‰å…¥
        time.sleep(2)
        
        # ç²å–é é¢å…§å®¹
        try:
            # å…ˆå˜—è©¦å¾ pre æ¨™ç±¤ä¸­æå– JSON
            try:
                pre_element = driver.find_element(By.TAG_NAME, "pre")
                json_text = pre_element.text
                print("âœ… å¾ pre æ¨™ç±¤ä¸­æ‰¾åˆ°å…§å®¹")
            except:
                # å¦‚æœæ²’æœ‰ pre æ¨™ç±¤ï¼Œæª¢æŸ¥ body å…§å®¹
                body_element = driver.find_element(By.TAG_NAME, "body")
                body_text = body_element.text
                
                # æª¢æŸ¥ body å…§å®¹æ˜¯å¦çœ‹èµ·ä¾†åƒ JSON
                if body_text.strip().startswith('{') and body_text.strip().endswith('}'):
                    json_text = body_text
                    print("âœ… å¾ body æ¨™ç±¤ä¸­æ‰¾åˆ° JSON å…§å®¹")
                else:
                    print("âŒ é é¢å…§å®¹ä¸æ˜¯ JSON æ ¼å¼")
                    print(f"Content-Type: {driver.execute_script('return document.contentType')}")
                    print(f"é é¢æ¨™é¡Œ: {driver.title}")
                    print(f"å›æ‡‰å…§å®¹: {body_text[:500]}...")
                    return None
            
            # å˜—è©¦è§£æ JSON
            data = json.loads(json_text)
            print("âœ… æˆåŠŸç²å– JSON è³‡æ–™")
            return data
                
        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æå¤±æ•—: {e}")
            print(f"å…§å®¹å‰500å­—å…ƒ: {json_text[:500]}...")
            return None
            
    except Exception as e:
        print(f"âŒ ç²å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None
    
    finally:
        driver.quit()

def process_reddit_data(data: Dict, description: str) -> Optional[Dict]:
    """è™•ç† Reddit è³‡æ–™"""
    try:
        if not data or 'data' not in data:
            print("âŒ ç„¡æ•ˆçš„ Reddit è³‡æ–™çµæ§‹")
            return None
        
        children = data.get('data', {}).get('children', [])
        total_posts = len(children)
        
        processed_data = {
            "updated": datetime.now().isoformat() + "Z",
            "source": description,
            "total_posts": total_posts,
            "original_data": data
        }
        
        print(f"âœ… è™•ç†å®Œæˆï¼ŒåŒ…å« {total_posts} ç¯‡æ–‡ç« ")
        return processed_data
        
    except Exception as e:
        print(f"âŒ è™•ç†è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def save_reddit_data(data: Dict, filename: str) -> Optional[Path]:
    """å„²å­˜ Reddit è³‡æ–™åˆ°æª”æ¡ˆ"""
    try:
        # ç¢ºä¿ data è³‡æ–™å¤¾å­˜åœ¨
        data_dir = Path("data")
        data_dir.mkdir(exist_ok=True)
        
        # å»ºç«‹å®Œæ•´æª”æ¡ˆè·¯å¾‘
        output_file = Path(filename)
        
        # å¯«å…¥ JSON æª”æ¡ˆ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è³‡æ–™å·²å„²å­˜è‡³: {output_file}")
        return output_file
        
    except Exception as e:
        print(f"âŒ å„²å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def scrape_all_reddit_data():
    """çˆ¬å–æ‰€æœ‰ Reddit å­ç‰ˆå¡Šè³‡æ–™"""
    results = []
    
    print("ğŸš€ é–‹å§‹çˆ¬å–æ‰€æœ‰ Reddit å­ç‰ˆå¡Š...")
    print("=" * 60)
    
    for reddit_config in REDDIT_URLS:
        print(f"\nğŸ“‹ è™•ç†: {reddit_config.description}")
        print("-" * 40)
        
        try:
            # ç²å–åŸå§‹è³‡æ–™
            raw_data = fetch_reddit_data_with_selenium(reddit_config.url)
            
            if raw_data:
                # è™•ç†è³‡æ–™
                processed_data = process_reddit_data(raw_data, reddit_config.description)
                
                if processed_data:
                    # å„²å­˜è³‡æ–™
                    output_file = save_reddit_data(processed_data, reddit_config.filename)
                    
                    if output_file:
                        results.append({
                            'description': reddit_config.description,
                            'filename': reddit_config.filename,
                            'posts_count': processed_data.get('total_posts', 0),
                            'status': 'success'
                        })
                    else:
                        results.append({
                            'description': reddit_config.description,
                            'filename': reddit_config.filename,
                            'status': 'save_failed'
                        })
                else:
                    results.append({
                        'description': reddit_config.description,
                        'filename': reddit_config.filename,
                        'status': 'process_failed'
                    })
            else:
                results.append({
                    'description': reddit_config.description,
                    'filename': reddit_config.filename,
                    'status': 'fetch_failed'
                })
        
        except Exception as e:
            print(f"âŒ è™•ç† {reddit_config.description} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            results.append({
                'description': reddit_config.description,
                'filename': reddit_config.filename,
                'status': 'error',
                'error': str(e)
            })
        
        # åœ¨å­ç‰ˆå¡Šä¹‹é–“æ·»åŠ å»¶é²
        if reddit_config != REDDIT_URLS[-1]:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹
            delay = random.uniform(3, 6)
            print(f"â³ ç­‰å¾… {delay:.2f} ç§’å¾Œç¹¼çºŒä¸‹ä¸€å€‹å­ç‰ˆå¡Š...")
            time.sleep(delay)
    
    return results

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ”¥ Reddit ç†±é–€æ–‡ç« çˆ¬èŸ² - Python ç‰ˆæœ¬")
    print("=" * 50)
    
    # çˆ¬å–æ‰€æœ‰è³‡æ–™
    results = scrape_all_reddit_data()
    
    # é¡¯ç¤ºç¸½çµ
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–çµæœç¸½çµ:")
    print("=" * 60)
    
    success_count = 0
    total_posts = 0
    
    for result in results:
        status = result['status']
        description = result['description']
        
        if status == 'success':
            posts_count = result.get('posts_count', 0)
            print(f"âœ… {description}: {posts_count} ç¯‡æ–‡ç« ")
            success_count += 1
            total_posts += posts_count
        else:
            print(f"âŒ {description}: {status}")
            if 'error' in result:
                print(f"   éŒ¯èª¤: {result['error']}")
    
    print("-" * 60)
    print(f"ğŸ¯ æˆåŠŸçˆ¬å–: {success_count}/{len(REDDIT_URLS)} å€‹å­ç‰ˆå¡Š")
    print(f"ğŸ“ˆ ç¸½æ–‡ç« æ•¸: {total_posts} ç¯‡")

if __name__ == "__main__":
    main()
