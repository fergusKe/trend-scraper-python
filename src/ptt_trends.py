#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PTT ç†±é–€æ–‡ç« çˆ¬èŸ² - Python ç‰ˆæœ¬
çˆ¬å– PTT 24å°æ™‚ç†±é–€æ–‡ç« è³‡æ–™
"""

import json
import time
import random
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

def setup_driver():
    """è¨­å®š Chrome WebDriver"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-setuid-sandbox')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-features=VizDisplayCompositor')
    
    # è¨­å®šæ›´çœŸå¯¦çš„ç”¨æˆ¶ä»£ç†
    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    options.add_argument(f'--user-agent={user_agent}')
    
    # è¨­å®šè¦–çª—å¤§å°
    options.add_argument('--window-size=1920,1080')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    # ç§»é™¤ webdriver ç—•è·¡
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

def extract_article_info(article_element, driver) -> Optional[Dict]:
    """å¾æ–‡ç« å…ƒç´ ä¸­æå–è³‡è¨Š"""
    try:
        article_data = {}
        
        # ä½¿ç”¨å¤šç¨®é¸æ“‡å™¨ç­–ç•¥ä¾†æå–æ¨æ–‡åˆ†æ•¸
        recommend_score = ""
        recommend_selectors = [
            ".e7-recommendScore",
            "[class*='recommendScore']",
            ".recommend-score"
        ]
        
        for selector in recommend_selectors:
            try:
                score_elem = article_element.find_element(By.CSS_SELECTOR, selector)
                recommend_score = score_elem.text.strip()
                if recommend_score:
                    break
            except NoSuchElementException:
                continue
        
        # æå–æ¨æ–‡æ•¸é‡
        recommend_count = ""
        count_selectors = [
            "[e7description='æ¨æ–‡:']",
            "[class*='recommendCount']",
            ".recommend-count"
        ]
        
        for selector in count_selectors:
            try:
                count_elem = article_element.find_element(By.CSS_SELECTOR, selector)
                # æ‰¾åˆ°çˆ¶å…ƒç´ ä¸­çš„æ•¸å­—
                parent = count_elem.find_element(By.XPATH, "..")
                count_text = parent.text.strip()
                count_match = re.search(r'(\d+)', count_text)
                if count_match:
                    recommend_count = count_match.group(1)
                    break
            except NoSuchElementException:
                continue
        
        # æå–æ¨™é¡Œå’Œé€£çµ
        title = ""
        link = ""
        title_selectors = [
            "a[href*='/bbs/']",
            ".title a",
            ".article-title a"
        ]
        
        for selector in title_selectors:
            try:
                title_elem = article_element.find_element(By.CSS_SELECTOR, selector)
                title = title_elem.text.strip()
                link = title_elem.get_attribute('href')
                if title and link:
                    # ç¢ºä¿é€£çµæ ¼å¼æ­£ç¢º
                    if link.startswith('/bbs/'):
                        link = link  # ä¿æŒç›¸å°è·¯å¾‘
                    break
            except NoSuchElementException:
                continue
        
        # æå–ä½œè€…
        author = ""
        author_selectors = [
            ".author",
            "[class*='author']",
            ".article-author"
        ]
        
        for selector in author_selectors:
            try:
                author_elem = article_element.find_element(By.CSS_SELECTOR, selector)
                author = author_elem.text.strip()
                if author:
                    break
            except NoSuchElementException:
                continue
        
        # å¾é€£çµä¸­æå–çœ‹æ¿åç¨±
        board = ""
        if link:
            board_match = re.search(r'/bbs/([^/]+)/', link)
            if board_match:
                board = board_match.group(1)
        
        # æå–ç™¼æ–‡æ™‚é–“
        publish_time = ""
        time_selectors = [
            ".publish-time",
            "[class*='publishTime']",
            ".article-time"
        ]
        
        for selector in time_selectors:
            try:
                time_elem = article_element.find_element(By.CSS_SELECTOR, selector)
                publish_time = time_elem.text.strip()
                if publish_time:
                    break
            except NoSuchElementException:
                continue
        
        # æå–åœ–ç‰‡ URLï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        image_url = ""
        try:
            img_elem = article_element.find_element(By.CSS_SELECTOR, "img")
            image_url = img_elem.get_attribute('src')
        except NoSuchElementException:
            pass
        
        # åªæœ‰ç•¶åŸºæœ¬è³‡è¨Šéƒ½å­˜åœ¨æ™‚æ‰è¿”å›è³‡æ–™
        if title and link:
            article_data = {
                "recommendScore": recommend_score,
                "recommendCount": recommend_count,
                "title": title,
                "link": link,
                "author": author,
                "board": board,
                "publishTime": publish_time
            }
            
            if image_url:
                article_data["imageUrl"] = image_url
            
            return article_data
        
    except Exception as e:
        print(f"âš ï¸ æå–æ–‡ç« è³‡è¨Šæ™‚å‡ºéŒ¯: {e}")
    
    return None

def smart_scroll(driver, target_count=20) -> None:
    """æ™ºæ…§æ»¾å‹•ç­–ç•¥ï¼šåˆå§‹ä¸æ»¾å‹•ä¿æŒé †åºï¼Œä¸è¶³20ç¯‡æ‰è¼•å¾®æ»¾å‹•è£œå……"""
    print("ğŸ“œ æª¢æŸ¥æ˜¯å¦éœ€è¦æ»¾å‹•è¼‰å…¥æ›´å¤šå…§å®¹...")
    
    # å…ˆæª¢æŸ¥ç•¶å‰æœ‰å¤šå°‘æ–‡ç« 
    articles = driver.find_elements(By.CSS_SELECTOR, ".e7-container, [class*='container']")
    current_count = len(articles)
    print(f"ğŸ“Š ç›®å‰æ‰¾åˆ° {current_count} ç¯‡æ–‡ç« ")
    
    if current_count >= target_count:
        print("âœ… æ–‡ç« æ•¸é‡å……è¶³ï¼Œç„¡éœ€æ»¾å‹•")
        return
    
    print("ğŸ“œ æ–‡ç« æ•¸é‡ä¸è¶³ï¼Œé–‹å§‹è¼•å¾®æ»¾å‹•è£œå……...")
    
    # è¼•å¾®æ»¾å‹•è£œå……
    scroll_attempts = 3
    for i in range(scroll_attempts):
        driver.execute_script("window.scrollBy(0, 800);")
        time.sleep(2)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ç« è¼‰å…¥
        new_articles = driver.find_elements(By.CSS_SELECTOR, ".e7-container, [class*='container']")
        new_count = len(new_articles)
        
        if new_count > current_count:
            print(f"ğŸ“ˆ æ»¾å‹•å¾Œå¢åŠ äº† {new_count - current_count} ç¯‡æ–‡ç« ")
            current_count = new_count
            if current_count >= target_count:
                break
        else:
            print(f"ğŸ“œ ç¬¬ {i+1} æ¬¡æ»¾å‹•æœªå¢åŠ æ–°å…§å®¹")

def scrape_ptt_trends():
    """çˆ¬å– PTT ç†±é–€æ–‡ç« """
    driver = setup_driver()
    articles = []
    
    try:
        print("ğŸš€ é–‹å§‹çˆ¬å– PTT ç†±é–€æ–‡ç« ...")
        
        # å‰å¾€ PTT ç†±é–€é é¢
        driver.get('https://www.pttweb.cc/hot/all/today')
        
        # ç­‰å¾…é é¢è¼‰å…¥
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            print("âœ… é é¢è¼‰å…¥å®Œæˆ")
        except TimeoutException:
            print("âš ï¸ é é¢è¼‰å…¥è¶…æ™‚")
        
        # ç­‰å¾…ä¸€æ®µæ™‚é–“è®“å‹•æ…‹å…§å®¹è¼‰å…¥
        time.sleep(3)
        
        # æ™ºæ…§æ»¾å‹•
        smart_scroll(driver, target_count=20)
        
        # å°‹æ‰¾æ–‡ç« å®¹å™¨
        article_selectors = [
            ".e7-container",
            "[class*='container']",
            ".article-item",
            ".hot-article"
        ]
        
        article_elements = []
        for selector in article_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    article_elements = elements
                    print(f"âœ… ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æ‰¾åˆ° {len(elements)} å€‹æ–‡ç« å…ƒç´ ")
                    break
            except Exception as e:
                print(f"âš ï¸ é¸æ“‡å™¨ '{selector}' å¤±æ•—: {e}")
                continue
        
        if not article_elements:
            print("âŒ ç„¡æ³•æ‰¾åˆ°æ–‡ç« å…ƒç´ ")
            return []
        
        print(f"ğŸ“Š é–‹å§‹è§£æ {len(article_elements)} å€‹æ–‡ç« ...")
        
        # è§£ææ¯ç¯‡æ–‡ç« 
        seen_titles = set()  # ç”¨æ–¼å»é‡
        
        for i, article_elem in enumerate(article_elements):
            try:
                article_data = extract_article_info(article_elem, driver)
                
                if article_data and article_data.get('title'):
                    # å»é‡æª¢æŸ¥
                    title = article_data['title']
                    if title not in seen_titles:
                        articles.append(article_data)
                        seen_titles.add(title)
                        print(f"âœ… ç¬¬ {len(articles)} ç¯‡: {title[:50]}...")
                    else:
                        print(f"ğŸ”„ é‡è¤‡æ–‡ç« å·²è·³é: {title[:30]}...")
                
                # é™åˆ¶æœ€å¤šçˆ¬å–30ç¯‡
                if len(articles) >= 30:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ è§£æç¬¬ {i+1} ç¯‡æ–‡ç« æ™‚å‡ºéŒ¯: {e}")
                continue
    
    except Exception as e:
        print(f"âŒ çˆ¬å–éç¨‹ä¸­å‡ºéŒ¯: {e}")
    
    finally:
        driver.quit()
    
    return articles

def save_ptt_data(articles):
    """å„²å­˜ PTT è³‡æ–™åˆ° JSON æª”æ¡ˆ"""
    data = {
        "updated": datetime.now().isoformat() + "Z",
        "total_found": len(articles),
        "returned_count": len(articles),
        "articles": articles
    }
    
    # ç¢ºä¿ data è³‡æ–™å¤¾å­˜åœ¨
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    
    # å¯«å…¥ JSON æª”æ¡ˆ
    output_file = data_dir / "ptt-trends.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ è³‡æ–™å·²å„²å­˜è‡³: {output_file}")
    return output_file

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ“° PTT ç†±é–€æ–‡ç« çˆ¬èŸ² - Python ç‰ˆæœ¬")
    print("=" * 50)
    
    # çˆ¬å–è³‡æ–™
    articles = scrape_ptt_trends()
    
    if articles:
        # å„²å­˜è³‡æ–™
        save_ptt_data(articles)
        
        # é¡¯ç¤ºçµæœæ‘˜è¦
        print("âœ… çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“Š ç¸½å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        # é¡¯ç¤ºå‰å¹¾ç¯‡æ–‡ç« æ¨™é¡Œ
        for i, article in enumerate(articles[:5], 1):
            print(f"{i}. {article.get('title', 'N/A')[:60]}...")
    else:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ")

if __name__ == "__main__":
    main()
